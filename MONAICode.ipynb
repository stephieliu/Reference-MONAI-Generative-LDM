{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Copyright (c) MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# CHANGES MADE:\n",
    "# Changed the seed from 1009 -> 42 (line 56)\n",
    "# Changed training model epochs from 150 -> 500\n",
    "# Changed training interval steps from 100 -> 1000\n",
    "# Changed diffusion model epochs from 150 -> 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from monai import transforms\n",
    "from monai.apps import DecathlonDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader\n",
    "from monai.utils import first, set_determinism\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import L1Loss\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "from generative.inferers import LatentDiffusionInferer\n",
    "from generative.losses import PatchAdversarialLoss, PerceptualLoss\n",
    "from generative.networks.nets import AutoencoderKL, DiffusionModelUNet, PatchDiscriminator\n",
    "from generative.networks.schedulers import DDPMScheduler\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility purposes set a seed\n",
    "set_determinism(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setup a data directory and download dataset\n",
    "directory = '../monaiData'\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prepare data loader for training set\n",
    "batch_size = 2\n",
    "channel = 0  # 0 = Flair\n",
    "assert channel in [0, 1, 2, 3], \"Choose a valid channel\"\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\"]),\n",
    "        transforms.EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        transforms.Lambdad(keys=\"image\", func=lambda x: x[channel, :, :, :]),\n",
    "        transforms.EnsureChannelFirstd(keys=[\"image\"], channel_dim=\"no_channel\"),\n",
    "        transforms.EnsureTyped(keys=[\"image\"]),\n",
    "        transforms.Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "        transforms.Spacingd(keys=[\"image\"], pixdim=(2.4, 2.4, 2.2), mode=(\"bilinear\")),\n",
    "        transforms.CenterSpatialCropd(keys=[\"image\"], roi_size=(96, 96, 64)),\n",
    "        transforms.ScaleIntensityRangePercentilesd(keys=\"image\", lower=0, upper=99.5, b_min=0, b_max=1),\n",
    "    ]\n",
    ")\n",
    "train_ds = DecathlonDataset(\n",
    "    root_dir=root_dir,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    section=\"training\",  # validation\n",
    "    cache_rate=1.0,  # you may need a few Gb of RAM... Set to 0 otherwise\n",
    "    num_workers=8,\n",
    "    download=False,  # Set download to True if the dataset hasnt been downloaded yet\n",
    "    seed=0,\n",
    "    transform=train_transforms,\n",
    ")\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=8, persistent_workers=True)\n",
    "print(f'Image shape {train_ds[0][\"image\"].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualise examples from the training set\n",
    "# Plot axial, coronal and sagittal slices of a training sample\n",
    "check_data = first(train_loader)\n",
    "idx = 0\n",
    "\n",
    "img = check_data[\"image\"][idx, 0]\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3)\n",
    "for ax in axs:\n",
    "    ax.axis(\"off\")\n",
    "ax = axs[0]\n",
    "ax.imshow(img[..., img.shape[2] // 2], cmap=\"gray\")\n",
    "ax = axs[1]\n",
    "ax.imshow(img[:, img.shape[1] // 2, ...], cmap=\"gray\")\n",
    "ax = axs[2]\n",
    "ax.imshow(img[img.shape[0] // 2, ...], cmap=\"gray\")\n",
    "plt.savefig(\"training_examples8.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining the autoencoder KL\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = AutoencoderKL(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    num_channels=(32, 64, 64),\n",
    "    latent_channels=3,\n",
    "    num_res_blocks=1,\n",
    "    norm_num_groups=16,\n",
    "    attention_levels=(False, False, True),\n",
    ")\n",
    "autoencoder.to(device)\n",
    "\n",
    "\n",
    "discriminator = PatchDiscriminator(spatial_dims=3, num_layers_d=3, num_channels=32, in_channels=1, out_channels=1)\n",
    "discriminator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining losses\n",
    "l1_loss = L1Loss()\n",
    "adv_loss = PatchAdversarialLoss(criterion=\"least_squares\")\n",
    "loss_perceptual = PerceptualLoss(spatial_dims=3, network_type=\"squeeze\", is_fake_3d=True, fake_3d_ratio=0.2)\n",
    "loss_perceptual.to(device)\n",
    "\n",
    "\n",
    "def KL_loss(z_mu, z_sigma):\n",
    "    kl_loss = 0.5 * torch.sum(z_mu.pow(2) + z_sigma.pow(2) - torch.log(z_sigma.pow(2)) - 1, dim=[1, 2, 3, 4])\n",
    "    return torch.sum(kl_loss) / kl_loss.shape[0]\n",
    "\n",
    "\n",
    "adv_weight = 0.01\n",
    "perceptual_weight = 0.001\n",
    "kl_weight = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_g = torch.optim.Adam(params=autoencoder.parameters(), lr=1e-4)\n",
    "optimizer_d = torch.optim.Adam(params=discriminator.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "n_epochs = 500\n",
    "autoencoder_warm_up_n_epochs = 5\n",
    "val_interval = 10\n",
    "epoch_recon_loss_list = []\n",
    "epoch_gen_loss_list = []\n",
    "epoch_disc_loss_list = []\n",
    "val_recon_epoch_loss_list = []\n",
    "intermediary_images = []\n",
    "n_example_images = 4\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    autoencoder.train()\n",
    "    discriminator.train()\n",
    "    epoch_loss = 0\n",
    "    gen_epoch_loss = 0\n",
    "    disc_epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=110)\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in progress_bar:\n",
    "        images = batch[\"image\"].to(device)  # choose only one of Brats channels\n",
    "\n",
    "        # Generator part\n",
    "        optimizer_g.zero_grad(set_to_none=True)\n",
    "        reconstruction, z_mu, z_sigma = autoencoder(images)\n",
    "        kl_loss = KL_loss(z_mu, z_sigma)\n",
    "\n",
    "        recons_loss = l1_loss(reconstruction.float(), images.float())\n",
    "        p_loss = loss_perceptual(reconstruction.float(), images.float())\n",
    "        loss_g = recons_loss + kl_weight * kl_loss + perceptual_weight * p_loss\n",
    "\n",
    "        if epoch > autoencoder_warm_up_n_epochs:\n",
    "            logits_fake = discriminator(reconstruction.contiguous().float())[-1]\n",
    "            generator_loss = adv_loss(logits_fake, target_is_real=True, for_discriminator=False)\n",
    "            loss_g += adv_weight * generator_loss\n",
    "\n",
    "        loss_g.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        if epoch > autoencoder_warm_up_n_epochs:\n",
    "            # Discriminator part\n",
    "            optimizer_d.zero_grad(set_to_none=True)\n",
    "            logits_fake = discriminator(reconstruction.contiguous().detach())[-1]\n",
    "            loss_d_fake = adv_loss(logits_fake, target_is_real=False, for_discriminator=True)\n",
    "            logits_real = discriminator(images.contiguous().detach())[-1]\n",
    "            loss_d_real = adv_loss(logits_real, target_is_real=True, for_discriminator=True)\n",
    "            discriminator_loss = (loss_d_fake + loss_d_real) * 0.5\n",
    "\n",
    "            loss_d = adv_weight * discriminator_loss\n",
    "\n",
    "            loss_d.backward()\n",
    "            optimizer_d.step()\n",
    "\n",
    "        epoch_loss += recons_loss.item()\n",
    "        if epoch > autoencoder_warm_up_n_epochs:\n",
    "            gen_epoch_loss += generator_loss.item()\n",
    "            disc_epoch_loss += discriminator_loss.item()\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            {\n",
    "                \"recons_loss\": epoch_loss / (step + 1),\n",
    "                \"gen_loss\": gen_epoch_loss / (step + 1),\n",
    "                \"disc_loss\": disc_epoch_loss / (step + 1),\n",
    "            }\n",
    "        )\n",
    "    epoch_recon_loss_list.append(epoch_loss / (step + 1))\n",
    "    epoch_gen_loss_list.append(gen_epoch_loss / (step + 1))\n",
    "    epoch_disc_loss_list.append(disc_epoch_loss / (step + 1))\n",
    "\n",
    "del discriminator\n",
    "del loss_perceptual\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curve\n",
    "plt.figure()\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.title(\"Learning Curves\", fontsize=20)\n",
    "plt.plot(epoch_recon_loss_list)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xlabel(\"Epochs\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)\n",
    "# plt.legend(prop={\"size\": 14})\n",
    "plt.savefig('learning_curves8.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "# writer.add_graph();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial Training Curve Plot\n",
    "plt.figure()\n",
    "plt.title(\"Adversarial Training Curves\", fontsize=20)\n",
    "plt.plot(epoch_gen_loss_list, color=\"C0\", linewidth=2.0, label=\"Generator\")\n",
    "plt.plot(epoch_disc_loss_list, color=\"C1\", linewidth=2.0, label=\"Discriminator\")\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xlabel(\"Epochs\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)\n",
    "plt.legend(prop={\"size\": 14})\n",
    "plt.savefig('adversarial_training_curve8.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstructions\n",
    "# Plot axial, coronal and sagittal slices of a training sample\n",
    "idx = 0\n",
    "img = reconstruction[idx, channel].detach().cpu().numpy()\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3)\n",
    "for ax in axs:\n",
    "    ax.axis(\"off\")\n",
    "ax = axs[0]\n",
    "ax.imshow(img[..., img.shape[2] // 2], cmap=\"gray\")\n",
    "ax = axs[1]\n",
    "ax.imshow(img[:, img.shape[1] // 2, ...], cmap=\"gray\")\n",
    "ax = axs[2]\n",
    "ax.imshow(img[img.shape[0] // 2, ...], cmap=\"gray\")\n",
    "plt.savefig('reconstructions8.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diffusion model\n",
    "# Define diffusion model and scheduler\n",
    "unet = DiffusionModelUNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    num_res_blocks=1,\n",
    "    num_channels=(32, 64, 64),\n",
    "    attention_levels=(False, True, True),\n",
    "    num_head_channels=(0, 64, 64),\n",
    ")\n",
    "unet.to(device)\n",
    "\n",
    "\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000, schedule=\"scaled_linear_beta\", beta_start=0.0015, beta_end=0.0195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling factor\n",
    "with torch.no_grad():\n",
    "    with autocast(enabled=True):\n",
    "        z = autoencoder.encode_stage_2_inputs(check_data[\"image\"].to(device))\n",
    "\n",
    "print(f\"Scaling factor set to {1/torch.std(z)}\")\n",
    "scale_factor = 1 / torch.std(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inferer using scaling factor\n",
    "inferer = LatentDiffusionInferer(scheduler, scale_factor=scale_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_diff = torch.optim.Adam(params=unet.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train diffusion model\n",
    "n_epochs = 500\n",
    "epoch_loss_list = []\n",
    "autoencoder.eval()\n",
    "scaler = GradScaler()\n",
    "\n",
    "first_batch = first(train_loader)\n",
    "z = autoencoder.encode_stage_2_inputs(first_batch[\"image\"].to(device))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    unet.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=70)\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in progress_bar:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        optimizer_diff.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(enabled=True):\n",
    "            # Generate random noise\n",
    "            noise = torch.randn_like(z).to(device)\n",
    "\n",
    "            # Create timesteps\n",
    "            timesteps = torch.randint(\n",
    "                0, inferer.scheduler.num_train_timesteps, (images.shape[0],), device=images.device\n",
    "            ).long()\n",
    "\n",
    "            # Get model prediction\n",
    "            noise_pred = inferer(\n",
    "                inputs=images, autoencoder_model=autoencoder, diffusion_model=unet, noise=noise, timesteps=timesteps\n",
    "            )\n",
    "\n",
    "            loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer_diff)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        progress_bar.set_postfix({\"loss\": epoch_loss / (step + 1)})\n",
    "    epoch_loss_list.append(epoch_loss / (step + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves plot\n",
    "plt.figure()\n",
    "plt.plot(epoch_loss_list)\n",
    "plt.title(\"Learning Curves\", fontsize=20)\n",
    "plt.plot(epoch_loss_list)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xlabel(\"Epochs\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)\n",
    "plt.legend(prop={\"size\": 14})\n",
    "plt.savefig('LDM_learning_curves8.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting sampling example\n",
    "autoencoder.eval()\n",
    "unet.eval()\n",
    "\n",
    "noise = torch.randn((1, 3, 24, 24, 16))\n",
    "noise = noise.to(device)\n",
    "scheduler.set_timesteps(num_inference_steps=1000)\n",
    "synthetic_images = inferer.sample(\n",
    "    input_noise=noise, autoencoder_model=autoencoder, diffusion_model=unet, scheduler=scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize synthesized data\n",
    "idx = 0\n",
    "img = synthetic_images[idx, channel].detach().cpu().numpy()  # images\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3)\n",
    "for ax in axs:\n",
    "    ax.axis(\"off\")\n",
    "ax = axs[0]\n",
    "ax.imshow(img[..., img.shape[2] // 2], cmap=\"gray\")\n",
    "ax = axs[1]\n",
    "ax.imshow(img[:, img.shape[1] // 2, ...], cmap=\"gray\")\n",
    "ax = axs[2]\n",
    "ax.imshow(img[img.shape[0] // 2, ...], cmap=\"gray\")\n",
    "plt.savefig('synthesized_data8.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up data\n",
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
